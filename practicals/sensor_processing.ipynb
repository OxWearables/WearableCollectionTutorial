{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844860da-c9de-47a7-81f8-72267a01d490",
   "metadata": {},
   "source": [
    "# Creating labelled wearable data-sets using wearable cameras\n",
    "\n",
    "In this practical, you will get to process, visualise and annotate wearable camera and accelerometer data. Timestamped images from wearable cameras allow us to label when participants were doing various physical activity behaviours. For instance, the [CAPTURE24 data-set](https://arxiv.org/abs/2402.19229) used wearable cameras alongside wrist-worn accelerometers to label accelerometer recordings captured in free-living settings from over 150 particpant. \n",
    "\n",
    "Labelled free-living data is essential for validating wearable behaviour measurement approaches, and can be identified with the naturalistic validation phase of Keadle et al.'s [framework for evaluating wearable devices](https://journals.lww.com/acsm-essr/FullText/2019/10000/A_Framework_to_Evaluate_Devices_That_Assess.3.aspx). However, labelled free-living data is also essential for\n",
    "training behaviour measurement approaches based on machine-learning, such as fine-tuning the models pretrained using [self-supervised learning](https://www.nature.com/articles/s41746-024-01062-3) that our group has developed. \n",
    "\n",
    "Hopefully, this notebook gives you a sense of the value of labelled free-living data-sets, and the effort that goes into creating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ca30e-d469-41d7-9fdb-f209f0ff0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and local scripts\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "# Local scripts\n",
    "import autographer\n",
    "from sensorplot import ImageData, TextData, ScalarData, VectorData, SensorPlot\n",
    "from annotate import notebook_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2c846-7e4e-4eff-97ee-674006d5829a",
   "metadata": {},
   "source": [
    "## 1.1. Process and visualise wearable camera data\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../assets/wearable_camera.jpeg\" alt=\"wearable devices on person\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "We start with time-stamped images captured by a chest-worn camera.\n",
    "\n",
    "We use the `prepare_camera_data` function to find all images captured by the camera, and extract their time-stamps, which are contained in the file names.\n",
    "For example, `NOR-000000-000000-20230201123025.JPG` was an image captured at `2023/02/01 12:30:25` (well, at least supposedly - we will get to this caveat soon). It uses the `get_img_times` function to extract the portion of the file with the date, and convert it to a datetime object. The output of this function is a pandas DataFrame, `img_df`, which contains the paths of images captured by a chest-camera and the time-stamps of when those images were captured.\n",
    "\n",
    "Now, a caveat to this is that a bug with the camera caused its clock to jump to 2023/02/01, resulting in a portion of the camera data, having the incorrect time-stamps. However, we still were able correct this issue, as you can see with `time_correction = pd.Timestamp(\"2024-07-12 10:47:05\") - pd.Timestamp(\"2023-02-01 13:31:47\")`. How were we able to do this?\n",
    "\n",
    "Hint:\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../assets/sleuth_image.jpeg\" alt=\"wearable devices on person\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_camera_data(path_to_images: str):\n",
    "    path_to_images = Path(path_to_images)\n",
    "    img_paths = list(\n",
    "        Path(path_to_images).glob(\"*.JPG\")\n",
    "    )  # glob finds all files which end with .JPG within the `path_to_images` directory.\n",
    "\n",
    "    assert len(img_paths) > 0, \"path_to_images does not contain any .JPG files!\"\n",
    "\n",
    "    # functionality to proceess the timestamps from the image paths\n",
    "    time_format = \"%Y%m%d%H%M%S\"\n",
    "\n",
    "    def get_img_times(paths):\n",
    "        return [datetime.strptime(path.parts[-1][18:32], time_format) for path in paths]\n",
    "\n",
    "    img_times = get_img_times(img_paths)\n",
    "\n",
    "    img_df = pd.DataFrame(\n",
    "        {\n",
    "            \"time\": img_times,\n",
    "            \"path\": img_paths,\n",
    "        }\n",
    "    )\n",
    "    # time correction to images taken on 2023-02-01 from camera bug\n",
    "    time_correction = pd.Timestamp(\"2024-07-12 10:47:05\") - pd.Timestamp(\n",
    "        \"2023-02-01 13:31:47\"\n",
    "    )\n",
    "    img_df.loc[\n",
    "        (img_df[\"time\"] <= pd.Timestamp(\"2024-01-01\")), \"time\"\n",
    "    ] += time_correction\n",
    "\n",
    "    # sort by time, and drop the old index\n",
    "    img_df.sort_values(\"time\", inplace=True)\n",
    "    img_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in camera data\n",
    "path_to_images = \"../pilot_data_2024/camera\"  # fill in path to images\n",
    "img_df = prepare_camera_data(path_to_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae30189",
   "metadata": {},
   "source": [
    "### Brief exercise\n",
    "Get to know the data! Here is a bare minimum selection of [pandas](https://pandas.pydata.org) functions:\n",
    "```python\n",
    "# get to know `img_df`, for example:\n",
    "img_df.head() # this will print the top five rows\n",
    "img_df.describe() # this will describe the times that the images \n",
    "img_df.columns # this will return the column names\n",
    "\n",
    "img_df[\"time\"] # or `img_df.time` selects the \"time\" column\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7810b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "...  # play around with `img_df`, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa96bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the data\n",
    "n_imgs_to_show = 400\n",
    "every_n_images = 10\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "figsize = (20, 10)\n",
    "\n",
    "# Plot images with timestamps\n",
    "plt.figure(figsize=figsize)\n",
    "small_img_paths, small_img_times = img_df.path.to_list(), img_df.time.to_list()\n",
    "\n",
    "for i, (img_path, img_time) in enumerate(\n",
    "    zip(\n",
    "        small_img_paths[:n_imgs_to_show:every_n_images],\n",
    "        small_img_times[:n_imgs_to_show:every_n_images],\n",
    "    ),\n",
    "    1,\n",
    "):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    img = Image.open(img_path)\n",
    "    img.resize\n",
    "    plt.imshow(img)\n",
    "    plt.title(img_time.strftime(\"%m/%d %Hh%Mm%Ss\"))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a38931",
   "metadata": {},
   "source": [
    "### Exercise: estimating camera coverage\n",
    "Can you calculate:\n",
    "- How many images were captured in total?\n",
    "- When did the camera start capturing images, and when did it stop?\n",
    "- Are there any large gaps of time within the recording? \n",
    "- What is the average difference in time between consecutive images, and what is the average frame rate?\n",
    "\n",
    "Tips:\n",
    "```python\n",
    "img_df.time.max() # Pandas columns, called Series, have methods such as .min(), .max(), which could be useful.\n",
    "times = img_df.time.to_numpy() # You can also convert times to a numpy array, allowing you to efficiently work out the differences between pairs of time-stamps\n",
    "times[1:] - times[:-1] # <= such as this, which returns a np.timedelta64 object: https://numpy.org/doc/stable/reference/arrays.datetime.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e186a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc. total number of images\n",
    "total_n_imgs = ...\n",
    "print(f\"Total number of images: {total_n_imgs}\")\n",
    "\n",
    "# Start and stop time of images\n",
    "start_time = ...\n",
    "stop_time = ...\n",
    "print(f\"Start time: {start_time}, stop time: {stop_time}\")\n",
    "\n",
    "# Document large gaps in the recordings\n",
    "gaps = [(...)]  # start, stop time\n",
    "\n",
    "# Calc. mean time intervals between images\n",
    "time_intervals = ...\n",
    "mean_time_interval = ...\n",
    "mean_frame_rate = ...\n",
    "print(f\"Mean time interval: {mean_time_interval}, mean frame rate: {mean_frame_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090155b-21b2-4897-a24e-26b503e5a908",
   "metadata": {},
   "source": [
    "## 1.2 Accelerometer processing\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../assets/wearable_wrist.jpeg\" alt=\"wearable devices on person\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "We now move on to the accelerometer data!\n",
    "\n",
    "`wrist_df`, `thigh_df` and `ankle_df` are pandas DataFrames containining data from accelerometers worn at self-evident places on the body. The accelerometer data was processed from `.CWA` using [actipy](https://actipy.readthedocs.io). Although we have the data already processed in .csv format, the function we used to process the raw data was:\n",
    "```python\n",
    "import actipy \n",
    "ax3_data, info = actipy.read_device(\n",
    "    \"path_to_raw_accelerometer_data.CWA\",\n",
    "    lowpass_hz=20,\n",
    "    calibrate_gravity=True,\n",
    "    detect_nonwear=True,\n",
    "    resample_hz=30,\n",
    ")\n",
    "```\n",
    "Let's read this data in and visualise it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in AX3 files\n",
    "wrist_path = \"../pilot_data_2024/wrist.csv\"  # fill in path to images\n",
    "thigh_path = \"../pilot_data_2024/ankle.csv\"  # fill in path to images\n",
    "ankle_path = \"../pilot_data_2024/thigh.csv\"  # fill in path to images\n",
    "\n",
    "wrist_df = pd.read_csv(wrist_path, parse_dates=[\"time\"], index_col=0)\n",
    "thigh_df = pd.read_csv(thigh_path, parse_dates=[\"time\"], index_col=0)\n",
    "ankle_df = pd.read_csv(ankle_path, parse_dates=[\"time\"], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5dbbe",
   "metadata": {},
   "source": [
    "### Brief exercise\n",
    "Get to know the data! To get you started, we have made a loop that iterates through the data-frames and describes summary statistics for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f557e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for placement, df in zip([\"Wrist\", \"Thigh\", \"Ankle\"], [wrist_df, thigh_df, ankle_df]):\n",
    "    print(\"=\" * 30, placement, \"=\" * 30)\n",
    "    # print out what you want to know here, for example\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise accelerometer data\n",
    "for placement, df in zip([\"Wrist\", \"Thigh\", \"Ankle\"], [wrist_df, thigh_df, ankle_df]):\n",
    "    df[[\"x\", \"y\", \"z\"]].plot()\n",
    "    # Optionally, figure out how to make the x-axis the timestamps\n",
    "    plt.title(f\"{placement}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80daf7",
   "metadata": {},
   "source": [
    "### Exercise: estimating accelerometer coverage\n",
    "As with the camera data, we need to do some exploratory data analysis and figure out the amount of data, the resolution and coverage. Let's focus on the `wrist_df`.\n",
    "\n",
    "Can you calculate:\n",
    "- How many time-stamped readings were captured in total?\n",
    "- When was the first and last reading?\n",
    "- Are there any large gaps of time within the recording? \n",
    "- What is the median difference in time between consecutive readings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5747440-e48c-45ac-9d2f-13fb7cdc4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc. total number of accelerometer readings\n",
    "total_n_accel = ...\n",
    "print(f\"Total number of wrist accelerometer readings: {total_n_accel}\")\n",
    "\n",
    "# Start and stop time of readings\n",
    "start_time = ...\n",
    "stop_time = ...\n",
    "print(f\"Start time: {start_time}, stop time: {stop_time}\")\n",
    "\n",
    "# Document large gaps in the recordings\n",
    "gaps = [(...)]  # start, stop time\n",
    "\n",
    "# Calc. mean time intervals between images\n",
    "time_intervals = ...\n",
    "median_time_interval = ...\n",
    "print(f\"Median time interval: {median_time_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553293b9",
   "metadata": {},
   "source": [
    "## 1.3 Visualise the camera and accelerometer data together\n",
    "We now convery all sensor data into numpy arrays to plot all of them together using a custom `SensorPlot` class that we wrote. For those interested, the full code for this is in the `scripts/sensorplot.py` file. Essentially, this class just contains code which plots each type of data correctly, and makes sure that their varying sampling rates are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e39538-9de2-4a73-b67d-cab784f9200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera\n",
    "image_datetimes = np.array(small_img_times, dtype=np.datetime64)\n",
    "image_paths = np.array(small_img_paths)\n",
    "\n",
    "# Accelerometer\n",
    "#   Wrist\n",
    "wrist_datetimes = wrist_df.time.to_numpy()\n",
    "wrist_accel = wrist_df[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "wrist_light = wrist_df[\"light\"].to_numpy()\n",
    "wrist_temp = wrist_df[\"temperature\"].to_numpy()\n",
    "\n",
    "#   Thigh\n",
    "thigh_datetimes = thigh_df.time.to_numpy()\n",
    "thigh_accel = thigh_df[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "\n",
    "#   Ankle\n",
    "ankle_datetimes = ankle_df.time.to_numpy()\n",
    "ankle_accel = ankle_df[[\"x\", \"y\", \"z\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting by creating a list of SensorData objects\n",
    "sensor_data = [\n",
    "    ImageData(\"Camera\", image_datetimes, image_paths, plot_x_ticks=True, img_zoom=0.1),\n",
    "    ScalarData(\"Wrist Temp.\", wrist_datetimes, wrist_temp, plot_x_ticks=False),\n",
    "    ScalarData(\"Wrist Light\", wrist_datetimes, wrist_light, plot_x_ticks=False),\n",
    "    VectorData(\n",
    "        \"Wrist Accel.\",\n",
    "        wrist_datetimes,\n",
    "        wrist_accel,\n",
    "        plot_x_ticks=False,\n",
    "        dim_names=[\"x\", \"y\", \"z\"],\n",
    "    ),\n",
    "    VectorData(\n",
    "        \"Thigh Accel.\",\n",
    "        thigh_datetimes,\n",
    "        thigh_accel,\n",
    "        plot_x_ticks=False,\n",
    "        dim_names=[\"x\", \"y\", \"z\"],\n",
    "    ),\n",
    "    VectorData(\n",
    "        \"Ankle Accel.\",\n",
    "        ankle_datetimes,\n",
    "        ankle_accel,\n",
    "        plot_x_ticks=10,\n",
    "        dim_names=[\"x\", \"y\", \"z\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "sv = SensorPlot(sensor_data)\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = [  # these are the start times of each sequence of consecutive images. Once we get to the corresponding stop time, there is a break before the next burst of consecutive images.\n",
    "    pd.Timestamp(\"2024-07-10T16:00:37\"),\n",
    "    pd.Timestamp(\"2024-07-11T15:18:36\"),\n",
    "    pd.Timestamp(\"2024-07-11T17:34:30\"),\n",
    "    pd.Timestamp(\"2024-07-12T09:45:43\"),\n",
    "    pd.Timestamp(\"2024-07-12T10:34:32\"),\n",
    "    pd.Timestamp(\"2024-07-12T16:08:15\"),\n",
    "]\n",
    "stop_times = [\n",
    "    pd.Timestamp(\"2024-07-10T16:05:30\"),\n",
    "    pd.Timestamp(\"2024-07-11T15:23:29\"),\n",
    "    pd.Timestamp(\"2024-07-11T17:39:23\"),\n",
    "    pd.Timestamp(\"2024-07-12T09:50:36\"),\n",
    "    pd.Timestamp(\"2024-07-12T10:39:30\"),\n",
    "    pd.Timestamp(\"2024-07-12T16:17:30\"),\n",
    "]\n",
    "\n",
    "start_times = pd.Series(start_times).to_numpy()\n",
    "duration = np.timedelta64(\n",
    "    1, \"m\"\n",
    ")  # set how long you want visualise, starting with each start_time\n",
    "\n",
    "for start_time in start_times:\n",
    "    print(\n",
    "        f\"Looking at data from {str(start_time)[11:19]} to {str(start_time + duration)[11:19] }\"\n",
    "    )\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = sv.plot_window(start_time, duration)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b9268",
   "metadata": {},
   "source": [
    "### Brief discussion\n",
    "- Does the readings from the diffent placed sensors make sense alongside the contextual information from the camera? \n",
    "- Can you spot transitions from inactivity to activity?\n",
    "- Can you think of sensible ways of ensuring that data from these different devices are synchronised?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f408bf6",
   "metadata": {},
   "source": [
    "## 2. Annotate the image data\n",
    "\n",
    "In order to annotate each image taken by the camera, we need a set of annotations to choose from. This set of possible annotations is called the annotation schema. For detailed annotations of activity intensity, we tend to use the [compendium of physical activity](https://pacompendium.com) to inform our annotations.\n",
    "\n",
    "We've put together a simple function using maptlotlib to allow you to label the image data inline in this jupyter notebook.\n",
    "This is implemented in the `notebook_annotation` function.\n",
    "    \n",
    "The following commands are used to navigate:\n",
    "- `next`/`.` - move to the next image (if there are any left, but only jumping one image along)\n",
    "- `prev`/`,` - move to the previous image (if there are any left, but only jumping one image along)\n",
    "- `copy`/`c` - copy the current annotation to the next image, and display the next image\n",
    "- `quit`/`q` - quit the loop, saving the annotations to the numpy array\n",
    "\n",
    "> To input an annotation, just type that annotation (or its shortcut) in to the box and hit enter.\n",
    "\n",
    "A particularly useful shortcut for quickly annotating the same activity multiple times is:\n",
    "- `copy N`, or `c N`, where `N` is an integer. This copies the last annotations to the next `N` images. \n",
    "\n",
    "To make annotation faster, you can define shortcuts for each label, so that you can just enter the shortcut as opposed to the whole label.\n",
    "For example, if your set of labels (which we call a schema) is:\n",
    "```python\n",
    "schema = {  # come up with a better schema\n",
    "    # shortcut: long name\n",
    "    \"s\": \"Sedentary\",\n",
    "    \"l\": \"Light\",\n",
    "    \"m\": \"MVPA\",\n",
    "}\n",
    "```\n",
    "Then, you can just type `s` and hit enter and it will label the current image as `Sedentary`.\n",
    "\n",
    "Importanty, you can choose to proceed with the example annotation schema provided below (Sedentary behaviour, Light physical activity, Moderate-to-vigorous physical activity), or come up with your own.\n",
    "\n",
    "### Annotation easter-eggs\n",
    "As you are going through this annotation, we have some questions:\n",
    "- Which haircut did the train conductor have?\n",
    "- Is the camera-wearer left or right handed.\n",
    "- What did the camera-wearer have for breakfast.\n",
    "- Which railway station does the camera-wearer pass through?\n",
    "- Which sculpture does the camera-wearer pause to photograph?\n",
    "- Who pauses to do a push-up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir_name = (\n",
    "    \"../raw_data/annotations/activites\"  # path to where annotations will be saved\n",
    ")\n",
    "\n",
    "schema = {  # come up with a better schema\n",
    "    # shortcut: long name\n",
    "    \"s\": \"Sedentary\",\n",
    "    \"l\": \"Light\",\n",
    "    \"m\": \"MVPA\",\n",
    "}\n",
    "\n",
    "notebook_annotation(\n",
    "    label_dir_name,  # Where to save the annotations\n",
    "    schema,  # The schema to annotate your data with\n",
    "    image_paths,\n",
    "    image_datetimes,\n",
    "    imgs_to_display=10,  # How many images to display at once\n",
    "    save_freq=10,  # How often to save the annotations\n",
    "    figsize=(30, 10),  # This can be made bigger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the saved annotations and add it to the SensorPlot\n",
    "annotations = np.load(label_dir_name + \"/labels.npy\", allow_pickle=True)\n",
    "sv.add_data(\n",
    "    TextData(\n",
    "        \"Annotations\", image_datetimes, annotations, plot_x_ticks=False, fontsize=10\n",
    "    ),\n",
    "    index=1,\n",
    "    height_ratio=0.3,\n",
    ")\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = np.timedelta64(1, \"m\")\n",
    "\n",
    "for start_time in start_times:\n",
    "    print(\n",
    "        f\"Looking at data from {str(start_time)[11:19]} to {str(start_time + duration)[11:19] }\"\n",
    "    )\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = sv.plot_window(start_time, duration)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56559a6d",
   "metadata": {},
   "source": [
    "### Exercise: time-use summaries\n",
    "Now that you have annotated the data-set (or at least made a valiant effort at annotating some of it), can you estimate how much time was spent in the different annotations?\n",
    "\n",
    "Tips:\n",
    "```python\n",
    "img_df[\"labels\"] = annotations # will create a new \"labels\" column which matches up the labels with their corresponding images\n",
    "img_df.labels.value_counts() # will give you a tally of how often each label was applied. Based on your previous findings about the capture-rate, you should be able to estimate time spent...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dec506",
   "metadata": {},
   "outputs": [],
   "source": [
    "...  # come on, almost there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-somalia",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"../assets/wearable_modelling.png\" alt=\"wearable devices on person\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Well done! The combination of your labels, and the accelerometer data we collected will allow us to validate measurement approaches, as well as train new measurement approaches. This is the basis of downstream epidemiological analyses, built on the back of labelled sensor data.\n",
    "\n",
    "### Discussion\n",
    "1) Think about what makes a good annotation schema. Should each image be uniquely described by a single label, or should multiple labels apply to each image? Should we use very descriptive labels, or just a few relevant ones?\n",
    "\n",
    "2) How can we deal with bias introduced by the annotator, including biases that arise from practical issues such as fatigue from annotating many images?\n",
    "\n",
    "3) Are there better approaches to collecting ground-truth data at scale? Weigh-in on the pros and cons of these alternatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
