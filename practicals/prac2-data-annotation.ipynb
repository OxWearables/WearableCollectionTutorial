{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844860da-c9de-47a7-81f8-72267a01d490",
   "metadata": {},
   "source": [
    "# Measuring behaviour from wearable cameras and accelerometers\n",
    "\n",
    "In this practical, you will learn how to download, process and annotate the camera data you have collected. You will also have the opportunity to use models developed by the Oxford Wearables Group to extract activity profiles from your own accelerometer data.\n",
    "\n",
    "Labelled free-living data is essential for training activity recognition models. By going through the process of annotating your own data, you will begin to understand some of the difficulties of attempting to reduce real life down to a small set of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-cleanup",
   "metadata": {},
   "source": [
    "# 0. Requirements\n",
    "Before starting this tutorial, we expect you to have the following installed:\n",
    "- A recent version of Python (≥ 3.10)\n",
    "- A recent version of Java (OpenJDK) (≥ 8)\n",
    "- The following Python packages:\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - jupyter notebook\n",
    "    - [pillow](https://pillow.readthedocs.io/en/stable/) (used for image processing) \n",
    "    - [actipy](https://pypi.org/project/actipy/) (used for processing accelerometer data)\n",
    "    \n",
    "We went through installing these in the previous practical, but in case you are having trouble here is a reminder:\n",
    "\n",
    "### Reminder\n",
    "1. Download & install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) (light-weight version of Anaconda).\n",
    "2. (Mac) Open the terminal. (Windows) Launch the **Miniconda Prompt**.\n",
    "3. Create a virtual environment:\n",
    "    ```shell\n",
    "    conda create -n wearables_practicals python=3.10 openjdk\n",
    "    ```\n",
    "    This creates a virtual environment called `wearables_practicals` with Python version 3.10 and Java OpenJDK. \n",
    "4. Activate the environment and install the required packages using pip:\n",
    "    ```shell\n",
    "    conda activate wearables_practicals\n",
    "    pip install numpy matplotlib notebook pillow actipy\n",
    "    ```\n",
    "\n",
    "*** Alternatively, to install java for actipy on the iMacs, you can use homebrew:\n",
    "```shell\n",
    "brew install java\n",
    "sudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ca30e-d469-41d7-9fdb-f209f0ff0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and local scripts\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import actipy  # for reading in accelerometer data\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "# Local scripts\n",
    "import autographer\n",
    "from sensorplot import ImageData, TextData, ScalarData, VectorData, SensorPlot\n",
    "from annotate import notebook_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2c846-7e4e-4eff-97ee-674006d5829a",
   "metadata": {},
   "source": [
    "# 1. Prepare camera and accelerometer data\n",
    "\n",
    "## 1.1 Camera processing\n",
    "\n",
    "Plug the Autographer wearable camera into the computer using the provided USB cable and make a note of its directory (e.g., `D:/` for Windows, and `/Volumes/` for Mac). If you are having trouble connecting the device or have chosen not to record your own data, please ask your tutors to provide you with pre-recorded data instead.\n",
    "\n",
    "Check that the Autographer pops up, i.e. no \"No such file or directory\" errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac945807-dcb1-4ac4-8a71-05a35658b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /Volumes/Autographer/DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-finance",
   "metadata": {},
   "source": [
    "Download the camera data to your computer. The images will be saved in the directory where you initially cloned the project, under \"raw_data/camera\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493e884-2e52-4b36-8786-26ba689fcd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "autographer.downloadData(\"/Volumes/Autographer/\", \"raw_data/camera/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-instrumentation",
   "metadata": {},
   "source": [
    "If the above was successful, you should uncomment and run the command below to erase your data from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24039d5-9329-4e38-a628-422fee894f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autographer.deleteCameraData(\"/Volumes/Autographer/\") # uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-nerve",
   "metadata": {},
   "source": [
    "Execute the following cell to visualise some of the captured images. Select how many images to display by changing the \"n_imgs_to_show\" variable, and specify how many rows and columns you'd like them to be displayed in with the variables \"n_rows\" and \"n_cols\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28285948-bd3f-4f82-b297-4f71d11a1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change display settings\n",
    "n_imgs_to_show = 10\n",
    "n_rows = 2\n",
    "n_cols = 5\n",
    "\n",
    "# Prepare a list of image paths along with their timestamps\n",
    "time_format = \"%Y%m%d_%H%M%S\"\n",
    "\n",
    "\n",
    "def get_img_times(paths):\n",
    "    return [datetime.strptime(path.parts[-1][17:32], time_format) for path in paths]\n",
    "\n",
    "\n",
    "small_img_paths = list(Path(\"../raw_data/camera/small\").glob(\"*.JPG\"))\n",
    "small_img_times = get_img_times(small_img_paths)\n",
    "\n",
    "tuples = list(zip(small_img_paths, small_img_times))\n",
    "tuples.sort(key=lambda x: x[0])\n",
    "\n",
    "small_img_paths, small_img_times = zip(*tuples)\n",
    "\n",
    "# Plot images with timestamps\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, (img_path, img_time) in enumerate(\n",
    "    zip(small_img_paths[:n_imgs_to_show], small_img_times[:n_imgs_to_show]), 1\n",
    "):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.imshow(Image.open(img_path))\n",
    "    plt.title(img_time.strftime(\"%Hh%Mm%Ss\"))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a38931",
   "metadata": {},
   "source": [
    "# Exercise: estimating camera coverage\n",
    "The `small_img_times` array contains the times that the images were captured at. \n",
    "Can you calculate:\n",
    "- How many images were captured in total?\n",
    "- When did the camera start capturing images, and when did it stop?\n",
    "- What is the average difference in time between consecutive images, and what is the average frame rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e186a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc. total number of images\n",
    "total_n_imgs = ...\n",
    "print(f\"Total number of images: {total_n_imgs}\")\n",
    "\n",
    "# Start and stop time of images\n",
    "start_time = ...\n",
    "stop_time = ...\n",
    "print(f\"Start time: {start_time}, stop time: {stop_time}\")\n",
    "\n",
    "# Calc. mean time intervals between images\n",
    "time_intervals = ...\n",
    "mean_time_interval = ...\n",
    "mean_frame_rate = ...\n",
    "print(f\"Mean time interval: {mean_time_interval}, mean frame rate: {mean_frame_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090155b-21b2-4897-a24e-26b503e5a908",
   "metadata": {},
   "source": [
    "## 1.2 Accelerometer processing\n",
    "\n",
    "Plug the Axivity AX3 accelerometer into the computer using the provided USB cable. Again, please check that the accelerometer pops up and take note of its name (it will be something like AX317_43923 under \"/Volumes\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecc4b3-38c9-41a0-b009-4083b8759018",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-salon",
   "metadata": {},
   "source": [
    "Copy the .CWA file from the accelerometer to the computer in the \"raw_data/accelerometer\" sub-directory. To do this you can write the correct accelerometer path below, uncomment and then execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42505dd3-0a2d-4a7f-8a7a-aae54e288b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# mv /Volumes/[INSERT_ACCELEROMETER_ID_HERE]/CWA-DATA.CWA ../raw_data/accelerometer/CWA-DATA.CWA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-mission",
   "metadata": {},
   "source": [
    "Use actipy to read and process the accelerometer data. If you are interested to find out more about the precise processing steps used, you can read the following paper [Large Scale Population Assessment of Physical Activity Using Wrist Worn Accelerometers: The UK Biobank Study (Doherty et al., 2017)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0169649). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5747440-e48c-45ac-9d2f-13fb7cdc4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerometer reading in\n",
    "ax3_data, info = actipy.read_device(\n",
    "    \"../raw_data/accelerometer/CWA-DATA.CWA\",\n",
    "    lowpass_hz=20,\n",
    "    calibrate_gravity=True,\n",
    "    detect_nonwear=True,\n",
    "    resample_hz=30,\n",
    ")\n",
    "\n",
    "# Plot the accelerometer data\n",
    "ax3_data[[\"x\", \"y\", \"z\"]].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553293b9",
   "metadata": {},
   "source": [
    "## 1.3 Visualise the camera and accelerometer data together\n",
    "\n",
    "Convert both sensor data into numpy arrays to allow subsequent plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e39538-9de2-4a73-b67d-cab784f9200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera\n",
    "image_datetimes = np.array(small_img_times, dtype=np.datetime64)\n",
    "image_paths = np.array(small_img_paths)\n",
    "\n",
    "# Accelerometer\n",
    "sensor_datetimes = ax3_data.index.to_numpy()\n",
    "accelerometer_readings = ax3_data[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "light_readings = ax3_data[\"light\"].to_numpy()\n",
    "temperature_readings = ax3_data[\"temperature\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-invite",
   "metadata": {},
   "source": [
    "Select an appropriate time window to display by specifying which image to start from (\"start_time\" variable) and how many minutes/seconds to display (\"duration\" variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting by creating a list of SensorData objects\n",
    "sensor_data = [\n",
    "    ImageData(\"Camera\", image_datetimes, image_paths, plot_x_ticks=True, img_zoom=0.22),\n",
    "    ScalarData(\n",
    "        \"Temperature\", sensor_datetimes, temperature_readings, plot_x_ticks=False\n",
    "    ),\n",
    "    ScalarData(\"Light\", sensor_datetimes, light_readings, plot_x_ticks=False),\n",
    "    VectorData(\n",
    "        \"Accelerometer\",\n",
    "        sensor_datetimes,\n",
    "        accelerometer_readings,\n",
    "        plot_x_ticks=10,\n",
    "        dim_names=[\"x\", \"y\", \"z\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "sv = SensorPlot(sensor_data)\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time window to plot\n",
    "start_time = image_datetimes[\n",
    "    50\n",
    "]  # change this to a sensible start time, e.g. image_datetimes[0]\n",
    "duration = np.timedelta64(\n",
    "    30, \"s\"\n",
    ")  # change this to a sensible duration, e.g. 5 minutes, i.e. np.timedelta64(5, 'm')\n",
    "\n",
    "print(\n",
    "    f\"Looking at data from {str(start_time)[11:19]} to {str(start_time + duration)[11:19] }\"\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = sv.plot_window(start_time, duration)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f408bf6",
   "metadata": {},
   "source": [
    "# 2. Annotate the image data\n",
    "\n",
    "In order to annotate each image taken by the camera, we need a set of annotations to choose from. This set of possible annotations is called the annotation schema. For detailed annotations of physical activity, we tend to use the [compendium of physical activity](https://sites.google.com/site/compendiumofphysicalactivities/Activity-Categories?authuser=0) to inform our annotation schema.\n",
    "\n",
    "We've put together a simple function using maptlotlib to allow you to label the image data inline in this jupyter notebook.\n",
    "This is implemented in the `notebook_annotation` function.    \n",
    "    \n",
    "The following commands are used to navigate:\n",
    "- `next`/`.` - move to the next N images (if there are any left, but only jumping one image along)\n",
    "- `prev`/`,` - move to the previous N images (if there are any left, but only jumping one image along)\n",
    "- `copy`/`c` - copy the current annotation to the next image, and display the next N images\n",
    "- `quit`/`q` - quit the loop, saving the annotations to the numpy array\n",
    "\n",
    "A particularly useful shortcut for quickly annotating the same activity multiple times is:\n",
    "- `copy N`, or `c N`, where `N` is an integer. This copies the last annotations to the next `N` images. \n",
    "\n",
    "To make annotation faster, you can define shortcuts for each label, so that you can just enter the shortcut as opposed to the whole label.\n",
    "\n",
    "Importanty, you can choose to proceed with the example annotation schema provided below (Sedentary behaviour, Light physical activity, Moderate-to-vigorous physical activity), or come up with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir_name = (\n",
    "    \"../raw_data/annotations/activites\"  # path to where annotations will be saved\n",
    ")\n",
    "\n",
    "schema = {  # come up with a better schema\n",
    "    # shortcut: long name\n",
    "    \"s\": \"Sedentary\",\n",
    "    \"l\": \"Light\",\n",
    "    \"m\": \"MVPA\",\n",
    "}\n",
    "\n",
    "notebook_annotation(\n",
    "    label_dir_name,  # Where to save the annotations\n",
    "    schema,  # The schema to annotate your data with\n",
    "    image_paths,\n",
    "    image_datetimes,\n",
    "    imgs_to_display=5,  # How many images to display at once\n",
    "    save_freq=10,  # How often to save the annotations\n",
    "    figsize=(30, 10),  # This can be made bigger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the saved annotations and add it to the SensorPlot\n",
    "annotations = np.load(label_dir_name + \"/labels.npy\", allow_pickle=True)\n",
    "sv.add_data(\n",
    "    TextData(\n",
    "        \"Annotations\", image_datetimes, annotations, plot_x_ticks=False, fontsize=10\n",
    "    ),\n",
    "    index=1,\n",
    "    height_ratio=0.3,\n",
    ")\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time window to plot\n",
    "start_time = image_datetimes[\n",
    "    0\n",
    "]  # change this to a sensible start time, e.g. image_datetimes[10]\n",
    "duration = np.timedelta64(\n",
    "    60, \"s\"\n",
    ")  # change this to a sensible duration, e.g. 5 minutes, i.e. np.timedelta64(5, 'm')\n",
    "\n",
    "print(\n",
    "    f\"Looking at data from {str(start_time)[11:19]} to {str(start_time + duration)[11:19] }\"\n",
    ")\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = sv.plot_window(start_time, duration)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-somalia",
   "metadata": {},
   "source": [
    "## Questions to discuss \n",
    "1) Think about what makes a good annotation schema. Should each image be uniquely described by a single label, or should multiple labels apply to each image?\n",
    "\n",
    "2) What are the advantages and disadvantages of choosing a detailed annotation schema with many labels?\n",
    "\n",
    "3) How can we deal with bias introduced by the annotator, including biases that arise from practical issues such as fatigue from annotating many images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-cause",
   "metadata": {},
   "source": [
    "## Excercise: Run pre-trained models on your own data, to classify your activity epochs\n",
    "\n",
    "Go to the [biobankAccelerometerAnalysis](https://github.com/OxWearables/biobankAccelerometerAnalysis) GitHub page and follow the installation and usage instructions carefully.\n",
    "\n",
    "Run the tool to produce summary movement statistics from your Axivity file (.cwa). How do the model predictions (acc_id-timeSeries.csv file) compare to your own annotations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1c725",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
